{"componentChunkName":"component---src-templates-blog-post-js","path":"/ai-chatdic/","result":{"data":{"site":{"siteMetadata":{"title":"SK플래닛 TECH TOPIC"}},"markdownRemark":{"id":"2631b44d-9e4a-53f9-addd-0d7a736cdeca","excerpt":"이 글은 SK플래닛 사내 AI 프로젝트 Chat DIC에서 AWS Bedrock의 Prompt Caching 기능을 활용해 쿼리 생성 속도와 비용을 최적화한 사례를 다룹니다. 1. 개요 Chat DIC는 사용자의 자연어 요청을 기반으로 사내 DB 스키마 정보를 Bedrock 모델에 전달해 원하는 SQL…","html":"<p>이 글은 SK플래닛 사내 AI 프로젝트 <strong>Chat DIC</strong>에서 AWS Bedrock의 Prompt Caching 기능을 활용해 <strong>쿼리 생성 속도와 비용을 최적화한 사례</strong>를 다룹니다.</p>\n<h1>1. 개요</h1>\n<p>Chat DIC는 사용자의 자연어 요청을 기반으로 사내 DB 스키마 정보를 Bedrock 모델에 전달해</p>\n<ul>\n<li>원하는 <strong>SQL 쿼리를 생성하거나</strong>,</li>\n<li>관련된 <strong>테이블과 필드를 탐색</strong></li>\n</ul>\n<p>할 수 있도록 도와주는 시스템입니다.\n하지만 이 과정에서 매번 전체 스키마 정보를 Prompt에 포함시키다 보니,\n토큰 수가 급격히 증가하고 <strong>Throttling</strong> 및 <strong>응답 지연</strong> 문제가 발생했습니다.\n이 문제를 해결하기 위해 Bedrock의 <strong>Prompt Caching</strong> 기능을 도입했습니다.</p>\n<hr>\n<h1>2. Prompt Caching이란?</h1>\n<p>Prompt Caching은 반복적으로 사용되는 프롬프트 문맥(예: system, tools 등)을 캐시에 저장하여\n<strong>모델 재계산을 줄이고</strong>, <strong>응답 지연 시간과 토큰 비용을 절감</strong>할 수 있는 Bedrock의 기능입니다.</p>\n<h3>주요 효과</h3>\n<ul>\n<li><strong>지연 시간 단축:</strong> Cache된 문맥은 매번 재연산할 필요가 없어 빠른 응답 가능</li>\n<li><strong>토큰 비용 절감:</strong> Cache에서 읽은 토큰은 낮은 요율이 적용되어 비용 절감</li>\n<li><strong>효율적 자원 활용:</strong> 불필요한 재요청 및 Throttling을 줄여 안정적인 API 사용 가능</li>\n</ul>\n<h3>Chat DIC 적용 이유</h3>\n<p>Chat DIC은 <strong>DB 스키마 정보(system + tools prompt)</strong> 가 매우 크기 때문에\n모델이 매번 이를 다시 처리하는 비효율이 존재했습니다.<br>\n이를 Prompt Caching을 통해 상시 유지함으로써, <strong>message prompt의 변경된 부분만 계산</strong>하도록 최적화했습니다.</p>\n<hr>\n<h1>3. 작동 원리</h1>\n<h3>Cache Checkpoint</h3>\n<ul>\n<li>캐시 저장 지점으로, prompt prefix(연속된 문맥 블록)를 지정</li>\n<li>Claude 3.7+ 기준 최소 1,024 tokens 필요 (Chat DIC는 Claude 4 사용, 4.5는 A/B Testing 중)</li>\n<li>예: 1,800 tokens에서 cachepoint 지정 시, 이후 cachepoint 지정은 2,048 tokens 도달 시 가능</li>\n</ul>\n<h3>TTL (Time To Live)</h3>\n<ul>\n<li>캐시 유효 기간은 <strong>5분</strong></li>\n<li>TTL 내 캐시 히트 발생 시, TTL은 자동으로 재설정됨</li>\n</ul>\n<h3>지원 API</h3>\n<ul>\n<li>Converse / ConverseStream</li>\n<li>InvokeModel / InvokeModelWithResponseStream</li>\n<li>Cross-region Inference 기능과 조합 가능</li>\n</ul>\n<h3>Prompt 관리</h3>\n<ul>\n<li>Console 및 API에서 프롬프트 생성·수정 시 캐싱 옵션 설정 가능</li>\n<li><code class=\"language-text\">system</code>, <code class=\"language-text\">messages</code>, <code class=\"language-text\">tools</code> 등의 필드에 적용 가능</li>\n<li>체크포인트 추가 시 모델별 최대 제한 존재</li>\n</ul>\n<hr>\n<h1>4. Chat DIC 시스템 구조 및 이슈</h1>\n<h3>(1) 초기 구조: AWS Gateway + Lambda</h3>\n<p>Bedrock API를 Lambda로 호출 <br/><br/>\n주요 이슈:</p>\n<ul>\n<li>Gateway Timeout 29초 → 90초 확장해도 불충분</li>\n<li>SSE 통신 미지원 (<code class=\"language-text\">stream</code> 옵션 불가)</li>\n<li>Throttling 빈번히 발생</li>\n<li>Lambda 환경에서 Prompt Caching 미지원</li>\n</ul>\n<p>=> 결과적으로 이 구조는 테스트용으로만 유지</p>\n<h3>(2) 개선 구조: AWS ALB + EC2</h3>\n<ul>\n<li>SSE 통신 지원 (<code class=\"language-text\">stream=True</code> 정상 작동)</li>\n<li>Timeout 3600초까지 설정 가능 (기본 600초 사용)</li>\n<li>여전히 Throttling 이슈가 간헐적으로 발생</li>\n</ul>\n<h3>(3) Prompt Caching 도입</h3>\n<ul>\n<li><code class=\"language-text\">system</code>과 <code class=\"language-text\">tools</code> 프롬프트에 캐시 적용</li>\n<li><code class=\"language-text\">messages</code> 영역은 사용자 입력이 매번 달라 적용 효율이 낮음</li>\n<li>CachePoint 한도 초과 방지를 위해 선택적으로 사용</li>\n</ul>\n<hr>\n<h1>5. 적용 코드 예시</h1>\n<h3>🧩 System Prompt Caching</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">system_prompts <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>  \n<span class=\"token keyword\">for</span> message <span class=\"token keyword\">in</span> chat_request<span class=\"token punctuation\">.</span>messages<span class=\"token punctuation\">:</span>  \n    <span class=\"token keyword\">if</span> message<span class=\"token punctuation\">.</span>role <span class=\"token operator\">!=</span> <span class=\"token string\">\"system\"</span><span class=\"token punctuation\">:</span>  \n        <span class=\"token comment\"># ignore system messages here  </span>\n  <span class=\"token keyword\">continue</span>  \n <span class=\"token keyword\">assert</span> <span class=\"token builtin\">isinstance</span><span class=\"token punctuation\">(</span>message<span class=\"token punctuation\">.</span>content<span class=\"token punctuation\">,</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span>  \n    system_prompts<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"text\"</span><span class=\"token punctuation\">:</span> message<span class=\"token punctuation\">.</span>content<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>  \n\n<span class=\"token comment\"># Prompt Caching</span>\n<span class=\"token keyword\">if</span> system_prompts<span class=\"token punctuation\">:</span>  \n    system_prompts<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"cachePoint\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"type\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"default\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>  \n  \n<span class=\"token keyword\">return</span> system_prompts</code></pre></div>\n<h3>🧰 Tools Prompt Caching</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># add tool config  </span>\n<span class=\"token keyword\">if</span> chat_request<span class=\"token punctuation\">.</span>tools<span class=\"token punctuation\">:</span>  \n    tool_list <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>self<span class=\"token punctuation\">.</span>_convert_tool_spec<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">.</span>function<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> t <span class=\"token keyword\">in</span> chat_request<span class=\"token punctuation\">.</span>tools<span class=\"token punctuation\">]</span>\n    tool_list<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"cachePoint\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"type\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"default\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># Prompt Caching</span>\n    tool_config <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"tools\"</span><span class=\"token punctuation\">:</span> tool_list<span class=\"token punctuation\">}</span></code></pre></div>\n<hr>\n<h1>6. Cache Hit 구조 (Chat DIC에서의 적용 결과)</h1>\n<table>\n<thead>\n<tr>\n<th>구분</th>\n<th>조건</th>\n<th>설명</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Cache Miss</strong></td>\n<td>누적 토큰 수 &#x3C; 1,024 OR 프롬프트 prefix 불일치 OR TTL(5분) 초과</td>\n<td>모델 재연산 발생</td>\n</tr>\n<tr>\n<td><strong>Cache Hit</strong></td>\n<td>누적 토큰 수 ≥ 1,024 AND 프롬프트 prefix 완전 일치 AND TTL(5분) 내 유효</td>\n<td>system, tools tokens 계산 절감</td>\n</tr>\n</tbody>\n</table>\n<h3>실제 효과</h3>\n<ul>\n<li>캐시 도입 전 평균 응답 시간: <strong>~29.3초</strong></li>\n<li>캐시 도입 후 평균 응답 시간: <strong>~23.1초</strong></li>\n<li>Throttling 발생률: <strong>약 60% 감소</strong></li>\n<li>시스템 리소스 부하 및 비용 절감 효과 확인</li>\n</ul>\n<hr>\n<h2>7. 마치며</h2>\n<p>Prompt Caching은 단순히 모델 호출 속도를 개선하는 기술이 아니라,\n<strong>LLM 기반 시스템의 구조적 효율을 향상시키는 핵심 기능</strong>입니다.</p>\n<p>Chat DIC 프로젝트의 경우,</p>\n<ul>\n<li>방대한 DB 스키마 정보를 캐싱하여 재사용함으로써 <strong>불필요한 토큰 소모를 줄이고</strong>,</li>\n<li><strong>SSE 스트리밍 환경에서도 안정적으로 작동</strong>하는 시스템을 구축할 수 있었습니다.</li>\n</ul>\n<p>향후에는 캐시 TTL 및 캐시 영역을 세분화하여\n사용자 맞춤형 Prompt 재활용 로직을 도입할 계획입니다.</p>\n<hr>\n<h2>8. 참고 링크</h2>\n<ul>\n<li>🔗 <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html\">AWS 공식 문서: Prompt Caching</a></li>\n<li>📘 SK Planet TechTopic: <a href=\"https://techtopic.skplanet.com/skp-techblog-intro/\">https://techtopic.skplanet.com/skp-techblog-intro/</a></li>\n</ul>","frontmatter":{"title":"Chat DIC 프로젝트에서 AWS Bedrock Prompt Caching으로 성능 최적화하기","date":"October 17, 2025","description":"사내 Chat DIC 프로젝트에서 AWS Bedrock을 이용해 DB 스키마 기반 쿼리 생성 서비스를 구현하며, Prompt Caching을 적용해 토큰 사용량과 응답 지연을 줄인 실제 사례를 소개합니다.","author":"rhs"}},"previous":{"fields":{"slug":"/openaiacademy/"},"frontmatter":{"title":"휴먼의 AI ＇학습＇은 이렇게! #1 - OpenAI Academy 사이트 톺아보기"}},"next":null},"pageContext":{"id":"2631b44d-9e4a-53f9-addd-0d7a736cdeca","previousPostId":"c4247250-c6dc-59c8-baa9-96a9b3009322","nextPostId":null}},"staticQueryHashes":["2841359383"],"slicesMap":{}}